{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4bb4fb9-c5ce-4454-84f8-dd5a365720ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating dataframe\n",
    "import pandas as pd\n",
    "\n",
    "db=pd.read_excel('DB.xlsx')\n",
    "# Create DataFrame\n",
    "df = db\n",
    "\n",
    "# Add a new column for suspect price\n",
    "df[\"suspect_price\"] = None\n",
    "\n",
    "# Initialize suspect price\n",
    "suspect_price = None\n",
    "\n",
    "# Iterate through each row\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"transaction_type\"] == \"suspect\":\n",
    "        suspect_price = row[\"price\"]  # Set suspect price\n",
    "    df.at[index, \"suspect_price\"] = suspect_price  # Add suspect price to the row\n",
    "\n",
    "#delete added column as index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Drop rows where suspect_price is None\n",
    "df.dropna(subset=[\"suspect_price\"], inplace=True)\n",
    "# Print the updated DataFrame\n",
    "df.to_excel('labeled_db.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b1881f-5db9-41cc-9612-b16f923b97ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m no_sus_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransaction_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuspect\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m no_sus_df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mno_sus_df.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "## creatinf df without suspect rows\n",
    "no_sus_df = df[df['transaction_type'] != 'suspect']\n",
    "no_sus_df.to_excel('no_sus_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecc7fcd-f495-460d-b5cc-8cfbbdad064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel('no_sus_df.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "487fea34-c2be-4aa1-9d54-9801ac506a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 10143.97483051181\n"
     ]
    }
   ],
   "source": [
    "###  1 by1 tagged input\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel('no_sus_df.xlsx')\n",
    "\n",
    "# Sample data (assuming df is your DataFrame)\n",
    "data = df\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "# Define features (X) and target (y) for train and test sets\n",
    "X_train = train_df[['price', 'transaction_type']]\n",
    "y_train = train_df['suspect_price']\n",
    "X_test = test_df[['price', 'transaction_type']]\n",
    "y_test = test_df['suspect_price']\n",
    "\n",
    "# Define preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('transaction', OneHotEncoder(), ['transaction_type'])\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create pipeline with preprocessing and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "95f5e2d6-f83c-42cc-a0de-ff3a97fe7577",
   "metadata": {},
   "outputs": [],
   "source": [
    "##evaluation previous code\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame for y_pred\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['Predicted_Suspect_Price'])\n",
    "\n",
    "# Convert y_test Series to DataFrame and reset index\n",
    "y_test_df = y_test.to_frame().reset_index(drop=True)\n",
    "\n",
    "# Concatenate X_test, y_pred_df, and y_test_df\n",
    "result_df = pd.concat([X_test.reset_index(drop=True), y_pred_df, y_test_df], axis=1)\n",
    "\n",
    "# Rename the y_test column to Actual_Suspect_Price for clarity\n",
    "result_df.rename(columns={'suspect_price': 'Actual_Suspect_Price'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "result_df.to_excel('predicted_results_1by1_tagged.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "913e5a25-99c7-412f-b854-bd9c282c19c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\sarimax.py:966: UserWarning: Non-stationary starting autoregressive parameters found. Using zeros as starting parameters.\n",
      "  warn('Non-stationary starting autoregressive parameters'\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "## arima model\n",
    "\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "df = pd.read_excel('no_sus_df.xlsx')\n",
    "\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "df.set_index('time', inplace=True)\n",
    "\n",
    "# Extract the time series (price)\n",
    "price_series = df['price']\n",
    "\n",
    "# Fit the ARIMA model\n",
    "model = ARIMA(price_series, order=(1, 0, 1))  # Example order (p, d, q)\n",
    "fit_model = model.fit()\n",
    "\n",
    "# Forecast the next suspect_price\n",
    "forecast = fit_model.forecast(steps=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62a3a48d-8791-4df9-8993-0c0bfd8c05e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Suspect Price for New Price Value: 15025.3637240175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:439: UserWarning: X does not have valid feature names, but LinearRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### one by one no tag input\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel('no_sus_df.xlsx')\n",
    "\n",
    "# Sample data (assuming df is your DataFrame)\n",
    "data = df\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "# Define features (X) and target (y) for train and test sets\n",
    "X_train = train_df[['price']]\n",
    "y_train = train_df['suspect_price']\n",
    "X_test = test_df[['price']]\n",
    "y_test = test_df['suspect_price']\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "##evaluation\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame for y_pred\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['Predicted_Suspect_Price'])\n",
    "\n",
    "# Convert y_test Series to DataFrame and reset index\n",
    "y_test_df = y_test.to_frame().reset_index(drop=True)\n",
    "\n",
    "# Concatenate X_test, y_pred_df, and y_test_df\n",
    "result_df = pd.concat([X_test.reset_index(drop=True), y_pred_df, y_test_df], axis=1)\n",
    "\n",
    "# Rename the y_test column to Actual_Suspect_Price for clarity\n",
    "result_df.rename(columns={'suspect_price': 'Actual_Suspect_Price'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "result_df.to_excel('predicted_results.xlsx', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2905e4df-abda-4ea6-a0a9-400a95125cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### one by one no tag , 2 feature input\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_excel('no_sus_df.xlsx')\n",
    "\n",
    "# Sample data (assuming df is your DataFrame)\n",
    "data = df\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define a function to map transaction types to numerical values\n",
    "def map_transaction_code(transaction_type):\n",
    "    if transaction_type == 'sell':\n",
    "        return 3\n",
    "    elif transaction_type == 'buy':\n",
    "        return -3\n",
    "    elif transaction_type == 'deal':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0  # Handle other cases if necessary\n",
    "\n",
    "# Add a new column 'transaction_code' with numerical representation\n",
    "df['transaction_code'] = df['transaction_type'].apply(map_transaction_code)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "# Define features (X) and target (y) for train and test sets\n",
    "X_train = train_df[['price','transaction_code']]\n",
    "y_train = train_df['suspect_price']\n",
    "X_test = test_df[['price','transaction_code']]\n",
    "y_test = test_df['suspect_price']\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "##evaluation\n",
    "\n",
    "# Convert the NumPy array to a pandas DataFrame for y_pred\n",
    "y_pred_df = pd.DataFrame(y_pred, columns=['Predicted_Suspect_Price'])\n",
    "\n",
    "# Convert y_test Series to DataFrame and reset index\n",
    "y_test_df = y_test.to_frame().reset_index(drop=True)\n",
    "\n",
    "# Concatenate X_test, y_pred_df, and y_test_df\n",
    "result_df = pd.concat([X_test.reset_index(drop=True), y_pred_df, y_test_df], axis=1)\n",
    "\n",
    "# Rename the y_test column to Actual_Suspect_Price for clarity\n",
    "result_df.rename(columns={'suspect_price': 'Actual_Suspect_Price'}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "result_df.to_excel('predicted_results_notag_2features.xlsx', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7faa2d32-03d5-4556-9f53-ae153154ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### one by one no tag , 2 feature input [random forest]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample data (assuming df is your DataFrame)\n",
    "data = df\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_size = int(len(df) * 0.8)\n",
    "train_df = df[:train_size]\n",
    "test_df = df[train_size:]\n",
    "\n",
    "# Define features (X) and target (y) for train and test sets\n",
    "X_train = train_df[['price', 'transaction_type']]\n",
    "y_train = train_df['suspect_price']\n",
    "X_test = test_df[['price', 'transaction_type']]\n",
    "y_test = test_df['suspect_price']\n",
    "\n",
    "# Initialize and train the Random Forest classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# You can use the trained model for further predictions or analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5219f22b-72a9-426a-8ca6-aefca10ba5a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20dc73ad-d915-4c12-8260-052bccda59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 221822560.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 221654000.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 221391168.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 221259888.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 221130976.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 221117632.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 220984496.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 220914384.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 220781760.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 220780816.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 220667152.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220634032.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220543536.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220383024.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220351040.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220281216.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 220172112.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219979648.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 219905024.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 220025600.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219848368.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 219796816.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219721760.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219616016.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219530160.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219513136.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 219356640.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 219319232.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 219219696.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 219167488.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 219059984.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218946208.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 218906080.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218866592.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218763856.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 218619904.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218507344.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 218671248.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218399728.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 218379824.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218189808.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 218153728.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 218215888.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 218038528.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 217874656.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 217872528.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 217831632.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 217745648.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 217632048.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 217546400.0000\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Predicted Suspect Price for New Sequence of Prices: 146.4421\n"
     ]
    }
   ],
   "source": [
    "### 10 by 10 no tag input\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "df = pd.read_excel('no_sus_df.xlsx')\n",
    "\n",
    "# Sample data (assuming df is your DataFrame)\n",
    "data = df\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "data['time'] = pd.to_datetime(data['time'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Define the sequence length\n",
    "sequence_length = 10\n",
    "\n",
    "# Initialize lists to store sequences and corresponding labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Create sequences of prices and corresponding labels\n",
    "for i in range(len(df) - sequence_length + 1):\n",
    "    sequences.append(df['price'].iloc[i:i+sequence_length])\n",
    "    labels.append(df['suspect_price'].iloc[i+sequence_length-1])\n",
    "\n",
    "# Convert lists to arrays\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(sequences, labels, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the input sequences for compatibility with LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(sequence_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Predict the suspect_price for new sequences\n",
    "# Example: assuming you have a new sequence of prices stored in a variable called new_sequence\n",
    "new_sequence = np.array([[15020, 15020, 15015, 15015, 15015, 15020, 15020, 15015, 15020, 15015]])  # Example sequence of prices\n",
    "new_sequence = scaler.transform(new_sequence)\n",
    "new_sequence = new_sequence.reshape(1, sequence_length, 1)\n",
    "new_pred = model.predict(new_sequence)\n",
    "print('Predicted Suspect Price for New Sequence of Prices:', new_pred[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a90d4cf-732b-4bf9-9da8-ce876217e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_4060\\3404299695.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  newest_rows['time'] = pd.to_datetime(newest_rows['time'])\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 219928896.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 219928608.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 219928320.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 219927968.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 219927632.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 219927216.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 219926784.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 219926320.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 219925808.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 219925216.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 219924592.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 219923856.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 219923056.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 219922128.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 219921072.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 219919920.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step - loss: 219918592.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - loss: 219917056.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 219915264.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - loss: 219913232.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 219910848.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 219908080.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 219904800.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - loss: 219900944.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - loss: 219896400.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 219891008.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 219884560.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 219876912.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 219867792.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 219857040.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 219844528.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 219830192.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - loss: 219814192.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 219796928.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 219779040.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 219761280.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - loss: 219744432.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 219729088.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 219715504.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 219703728.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 219693504.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 219684656.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 219676864.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 219669920.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 219663600.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 219657840.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 219652496.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 219647488.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 219642784.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 219638336.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 233ms/step\n",
      "Predicted Suspect Price for New Sequence of Prices: 9.942159\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "dff = pd.read_excel('no_sus_df.xlsx') \n",
    "\n",
    "# Select the 100 newest rows\n",
    "df = dff.tail(100)\n",
    "\n",
    "\n",
    "# Select the 10 newest rows\n",
    "newest_rows = df.tail(10)\n",
    "\n",
    "# Convert 'time' column to datetime\n",
    "newest_rows['time'] = pd.to_datetime(newest_rows['time'])\n",
    "\n",
    "# Define the sequence length\n",
    "sequence_length = 10\n",
    "\n",
    "# Initialize lists to store sequences and corresponding labels\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# Create sequences of prices and corresponding labels\n",
    "for i in range(len(newest_rows) - sequence_length + 1):\n",
    "    sequences.append(newest_rows['price'].iloc[i:i+sequence_length])\n",
    "    labels.append(newest_rows['suspect_price'].iloc[i+sequence_length-1])\n",
    "\n",
    "# Convert lists to arrays\n",
    "sequences = np.array(sequences)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Standardize the input features\n",
    "scaler = StandardScaler()\n",
    "sequences_scaled = scaler.fit_transform(sequences)\n",
    "\n",
    "# Reshape the input sequences for compatibility with LSTM\n",
    "sequences_reshaped = sequences_scaled.reshape(sequences_scaled.shape[0], sequences_scaled.shape[1], 1)\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(50, input_shape=(sequence_length, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(sequences_reshaped, labels, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Predict on the most recent sequence\n",
    "new_sequence = newest_rows['price'].values.reshape(1, sequence_length)  # Remove the extra dimension\n",
    "new_sequence_scaled = scaler.transform(new_sequence)\n",
    "new_sequence_scaled = new_sequence_scaled.reshape(1, sequence_length, 1)  # Reshape back to 3D for LSTM input\n",
    "new_pred = model.predict(new_sequence_scaled)\n",
    "print('Predicted Suspect Price for New Sequence of Prices:', new_pred[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c91f5-a5fa-4e13-808e-d69cb6e2cc8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
